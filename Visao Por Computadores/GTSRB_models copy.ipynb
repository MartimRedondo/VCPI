{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8246d4c1",
   "metadata": {},
   "source": [
    "# Teste dos Modelos sem Ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchinfo\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('src'))\n",
    "\n",
    "import src.vcpi_util as vcpi_util\n",
    "from src.constants import device, BATCH_SIZE, WORKERS, PREFETCH, EPOCHS\n",
    "from src.models import ModernCNN, ImprovedCNN, EfficientCNN, AttentionCNN, train_model, evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAINING_SET = Path(\"data/train_images\").resolve()\n",
    "PATH_TEST_SET = Path(\"data/test_images\").resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefef586",
   "metadata": {},
   "source": [
    "As transformações não são utilizadas neste contexto para testar a eficácia isolada do modelo.\n",
    "\n",
    "Apenas o redimensionamento é aplicado uma vez que o modelo exige uma entrada de tamanho fixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e19e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.Resize((32,32)), torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b752f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.ImageFolder(root=PATH_TRAINING_SET, transform = transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=WORKERS,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=PREFETCH,\n",
    "    persistent_workers=True)\n",
    "\n",
    "\n",
    "test_set = torchvision.datasets.ImageFolder(root=PATH_TEST_SET, transform = transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=WORKERS,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=PREFETCH,\n",
    "    persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4dff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def show_bad_preds(model, dataset, classes):\n",
    "    k = 0\n",
    "    iters = 0\n",
    "\n",
    "    preds = []\n",
    "    ground_truth = []\n",
    "    imgs = torch.Tensor(52, 3, 128,128)\n",
    "\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    max_iters = test_set.__len__() / BATCH_SIZE\n",
    "    while k < 52 and iters < max_iters:\n",
    "\n",
    "        images, targets = next(iterator)\n",
    "        #print(images[0].shape)\n",
    "        logits = model(images.to(device))\n",
    "        \n",
    "        #print(predictions[0])\n",
    "        predictions = torch.nn.functional.softmax(logits, dim=1).cpu().detach().numpy()\n",
    "        for i in range(len(predictions)):\n",
    "\n",
    "            if np.argmax(predictions[i]) != targets[i] and k < 52:\n",
    "\n",
    "                preds.append(predictions[i])\n",
    "                ground_truth.append(targets[i])\n",
    "                imgs[k, :, :, :] = inverse_normalize(images[i],[3, 128, 128], [0.229, 0.224, 0.225])\n",
    "                k += 1\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    vcpi_util.plot_predictions(imgs, preds, ground_truth, classes, 13, 4)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_info(model, train_loader, train_set, test_loader, EPOCHS, name=\"AttentionCNN\"):\n",
    "    model.to(device)\n",
    "\n",
    "    torchinfo.summary(model, input_size=(BATCH_SIZE, 3, 32, 32))\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    history = train_model(model, train_loader, test_loader, EPOCHS, loss_fn, optimizer)\n",
    "\n",
    "    best_train = np.asarray(history['train_acc']).argmax()\n",
    "    best_test = np.asarray(history['val_acc']).argmax()\n",
    "\n",
    "    print('Best epoch for train accuracy: :', best_train,' Best epoch for test accuracy: :',  best_test)\n",
    "    print('Test accuracy at epoch ',best_train, ' :', history['train_acc'][best_train], 'Test accuracy at epoch ',best_test, ' :', history['train_acc'][best_test])\n",
    "    preds = []\n",
    "    ground_truth = []\n",
    "\n",
    "    for images, targets in test_loader:\n",
    "\n",
    "        logits = model(images.to(device))\n",
    "        preds_sparse = [np.argmax(x) for x in logits.cpu().detach().numpy()]\n",
    "        preds.extend(preds_sparse)\n",
    "        ground_truth.extend(targets.numpy())\n",
    "\n",
    "    vcpi_util.show_confusion_matrix(ground_truth, preds, len(train_set.class_to_idx))\n",
    "\n",
    "    evaluate_model(model, test_loader)\n",
    "\n",
    "    vcpi_util.show_history_plus(history, ['train_acc', 'val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886125ad",
   "metadata": {},
   "source": [
    "# Modelo Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionCNN(len(train_set.classes))\n",
    "\n",
    "model_info(model, train_loader, train_set, test_loader, EPOCHS, name=\"AttentionCNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0841341",
   "metadata": {},
   "source": [
    "# Modelo Modern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7120b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModernCNN(len(train_set.classes))\n",
    "model.to(device)\n",
    "\n",
    "model_info(model, train_loader, train_set, test_loader, EPOCHS, name=\"ModernCNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1707f0",
   "metadata": {},
   "source": [
    "# Modelo Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d04a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImprovedCNN(len(train_set.classes))\n",
    "model.to(device)\n",
    "\n",
    "model_info(model, train_loader, train_set, test_loader, EPOCHS, name=\"ImprovedCNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5caa7d2",
   "metadata": {},
   "source": [
    "# Modelo Efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientCNN(len(train_set.classes))\n",
    "model.to(device)\n",
    "\n",
    "model_info(model, train_loader, train_set, test_loader, EPOCHS, name=\"EfficientCNN\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
