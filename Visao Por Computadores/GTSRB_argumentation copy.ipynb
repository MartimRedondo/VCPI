{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd8defb",
   "metadata": {},
   "source": [
    "## 1 - Configuração do Ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e67929",
   "metadata": {},
   "source": [
    "#### O notebook começa com a importação das bibliotecas necessárias e configuração do ambiente de trabalho.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import sys\n",
    "\n",
    "# Add src directory to sys.path so imports work\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('src'))\n",
    "\n",
    "import src.vcpi_util as vcpi_util\n",
    "from src.constants import device, BATCH_SIZE, WORKERS, PREFETCH\n",
    "from src.transformations import test_transform, train_transform, train_transform_advanced, train_transform_aggressive, train_transform_basic, train_transform_color, train_transform_geometric\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d439356",
   "metadata": {},
   "source": [
    "## 2 - Configurações de Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bea9dc",
   "metadata": {},
   "source": [
    "    • train_transform_basic: Transformações básicas como redimensionamento e normalização\n",
    "\n",
    "    • train_transform_geometric: Transformações geométricas como rotação, translação e zoom\n",
    "\n",
    "    • train_transform_color: Transformações de cor como brilho, contraste e saturação\n",
    "\n",
    "    • train_transform_aggressive: Combinação intensiva de transformações geométricas e de cor\n",
    "\n",
    "    • train_transform_advanced: Transformações avançadas com probabilidades ajustadas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_configs = {\n",
    "    'default': train_transform,\n",
    "    'basic': train_transform_basic,\n",
    "    'geometric': train_transform_geometric,\n",
    "    'color': train_transform_color,\n",
    "    'aggressive': train_transform_aggressive,\n",
    "    'advanced': train_transform_advanced\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233c265",
   "metadata": {},
   "source": [
    "## 3 - Carregamento e Exploração dos Dado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692dbe7",
   "metadata": {},
   "source": [
    "#### O notebook carrega o dataset GTSRB e exibe informações sobre o mesmo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f44f2",
   "metadata": {},
   "source": [
    "    • Número de amostras de treino: 35347\n",
    "\n",
    "    • Número de amostras de teste: 12630\n",
    "\n",
    "    • Número de classes: 43\n",
    "\n",
    "    • Lista das classes (IDs dos sinais de trânsito)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with augmentations\n",
    "data_path = 'data/'  # Path to the data folder\n",
    "\n",
    "# Load training dataset with augmentation\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=data_path + 'train_images',\n",
    "    transform=train_transform\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=WORKERS,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=PREFETCH,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=data_path + 'test_images',\n",
    "    transform=test_transform\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=WORKERS,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=PREFETCH,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# Print dataset information\n",
    "print(f'Number of training samples: {len(train_dataset)}')\n",
    "print(f'Number of test samples: {len(test_dataset)}')\n",
    "print(f'Number of classes: {len(train_dataset.classes)}')\n",
    "print(f'Classes: {train_dataset.classes}')\n",
    "\n",
    "# Display a few training images\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "vcpi_util.show_images(4, 8, images, labels, train_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f9119",
   "metadata": {},
   "source": [
    "## 4 - Visualização de Imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e11637",
   "metadata": {},
   "source": [
    "Função auxiliar para visualizar o efeito do data augmentation aplicado às imagens do dataset GTSRB. Esta função pode ser utilizada para verificar como as transformações afetam as imagens antes de treinar os modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar imagens com uma transformação específica\n",
    "def load_images(transformation_name='default'):\n",
    "    # Criar datasets com a augmentation atual\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path + 'train_images',\n",
    "        transform=augmentation_configs[transformation_name]\n",
    "    )\n",
    "\n",
    "    # Criar DataLoaders\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        num_workers=WORKERS,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=PREFETCH,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    return dataset, loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec1c06",
   "metadata": {},
   "source": [
    " **Default** |\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb127a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, loader = load_images(\"default\")\n",
    "images, labels = next(iter(loader))\n",
    "vcpi_util.show_images(4, 8, images, labels, dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e47e0c",
   "metadata": {},
   "source": [
    "**Basic**|\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bf167",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, loader = load_images(\"basic\")\n",
    "images, labels = next(iter(loader))\n",
    "vcpi_util.show_images(4, 8, images, labels, dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8807148",
   "metadata": {},
   "source": [
    "**Geometric**|\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bae1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, loader = load_images(\"geometric\")\n",
    "images, labels = next(iter(loader))\n",
    "vcpi_util.show_images(4, 8, images, labels, dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff30eac6",
   "metadata": {},
   "source": [
    "**Color**|\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f82f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, loader = load_images(\"color\")\n",
    "images, labels = next(iter(loader))\n",
    "vcpi_util.show_images(4, 8, images, labels, dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ac3ac",
   "metadata": {},
   "source": [
    "**Agressive**|\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, loader = load_images(\"aggressive\")\n",
    "images, labels = next(iter(loader))\n",
    "vcpi_util.show_images(4, 8, images, labels, dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f701e",
   "metadata": {},
   "source": [
    "**Advanced**|\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793969ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, loader = load_images(\"advanced\")\n",
    "images, labels = next(iter(loader))\n",
    "vcpi_util.show_images(4, 8, images, labels, dataset.classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
